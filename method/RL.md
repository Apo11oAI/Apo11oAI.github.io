环境状态 ==》行为 ==》累计奖励值最大
# 特征
1. 闭环
2. 没有直接指示采取什么行动（硬编码）
3. 行动的后果，包括奖励信号，在较长时间内发挥作用
# 原理
## 条件反射
## 马尔科夫决策过程（markov decision process）
对RL问题进行建模
1. S为所有环境状态的集合
2. A为agent可执行动作的集合
3. 策略π为状态空间到动作空间的**一个**映射
3. ρ为奖赏函数S×A → R 
```
r<sub>t</sub>~ρ（s<sub>t</sub>，a<sub>t</sub>）
agent在状态s<sub>t</sub>执行动作a<sub>t</sub>获得的立即奖赏值。
```
4. f:S×A×S → 【0,1】为状态转移概率分布函数
```
s<sub>t+1</sub>~ρ（s<sub>t</sub>，a<sub>t</sub>）
agent 在状态s<sub>t</sub>执行动作a<sub>t</sub>转移到下一状态s<sub>t+1</sub>的概率。
```
# 基本思想
通过最大化智能体从环境中获得的累计奖励值，以学习到完成目标的最优策略
RL方法更侧重于学习解决问题的策略
# 状态表示（环境建模）
环境模型模拟了环境的行为，给定一个状态和动作，模型可以预测下一个状态和奖赏。
## 环境特征
accessible vs. inaccessible
deterministic vs. non-determinitic
episodic vs. non-episodic
static vs. dynamic
discrete vs. continuous
## 时间维度
过去
现在（重点）
未来
## 主体维度
环境
agent



# 行为（选择）
## 探索（试错）
尝试各种动作即为试错
## 强化（利用）
趋近于好的动作
## 总体趋势
探索变少，利用变多
## 策略
agent在给定时间内的行为方式
从**环境感知的状态**到在**这些状态中可采用动作**的映射。

# 奖励
为使行为不是随意的，添加奖励条件限制agent行为,也就是目标导向。
## 奖赏函数
强化学习问题的目标
## 值函数

# 深度强化学习
## 学习过程
1. 在每个时刻agent与环境交互得到一个高维观察，并利用DL方法来感知观察，已得到抽象、具体的状态特征；
2. 基于预期回报来评价各个动作的价值函数，并通过某种策略将当前状态映射为相应的动作。
3. 环境对此动作做出反应，并得到